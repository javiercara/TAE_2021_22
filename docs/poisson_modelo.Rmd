---
title: 'Modelo de regresión de Poisson'
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
---

# Modelo

Durante la guerra del Vietnam, la armada de los Estados Unidos empleó diferentes tipos de bombarderos en misiones para destruir puentes, carreteras, y otros objetivos de transporte. Entre ellos destacaban el A-4 Skyhawk y el A-6 Intruder. El archivo *Aircraft_Damage.csv* contiene la información 30 misiones donde participaron estos aviones:

```{r}
d = read.csv("datos/Aircraft_Damage.csv")
str(d)
```

- damage: numero de zonas donde el bombardero presentaba daños debido a las defensas antibombarderos;
- bomber: variable cualitativa, vale 0 para el A-4 y 1 para el A-6;
- load: carga de bombas (en toneladas);
- experience: número de meses de experiencia en vuelo de la tripulación.

El objetivo es utilizar un modelo que relacione el número de zonas con daño y el resto de variables. En este caso la variable respuesta no es normal; y tampoco es binomial. Cuando la variable respuesta sea del tipo "número de veces que ocurre algo" se utiliza una variable de Poisson. 

El modelo de Poisson se define como

$$
P(Y_i = y_i) = \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!}, \quad y_i = 0,1,2,3,\ldots
$$

donde:

$$
\lambda_i = exp(\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_k x_{ki}) = exp(x_i^T \beta)
$$

ya que

$$
\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_k x_{ki} =
\begin{bmatrix}
1 & x_{1i} & x_{2i} & \cdots & x_{ki}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \cdots \\ \beta_k
\end{bmatrix}
= x_i^T \beta
$$

Otra forma de expresarlo es:

$$
Y_i = f(x_{1i}, x_{2i}, \cdots, x_{ki}) + u_i
$$

donde $y_i = 0,1,2,3,\cdots$. Se admite que $E[u_i] = 0$:

$$
E[y_i] = f(x_{1i}, x_{2i}, \cdots, x_{ki})
$$

Las variables de Poisson cumplen que $E[Y_i] = \lambda_i$, por lo que:

$$
f(x_{1i}, x_{2i}, \cdots, x_{ki}) = exp(x_i^T \beta)
$$

# Estimación de los parámetros del modelo: máxima verosimilitud

## La función de verosimilitud

La función de verosimilitud es la probabilidad de obtener la muestra dada. Por tanto, dada la muestra $\{y_1,y_2, \cdots, y_n \}$, la probabilidad de obtener dicha muestra es:

$$
P(Y_1 = y_1, Y_2 = y_2, \cdots, Y_n = y_n) = \prod_{i=1}^{n} P(Y_i = y_i) = \prod_{i=1}^{n} \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!} 
$$

Se denomina función de verosimilitud a la probabilidad de obtener la muestra:

$$
L(\beta) = \prod_{i=1}^{n} \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!}
$$

El logaritmo de la función de verosimilitud es:

$$
log L(\beta) = log \prod_{i=1}^{n} \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!} = \sum_{i=1}^{n}(-\lambda_i + y_i log(\lambda_i) + log(y_i!))
$$
En R, la función de verosimilitud la podemos calcular así:

```{r}
logL = function(beta,y,X){
  # asumimos que beta es un vector 
  # beta = [beta0 beta1 .. betak]
  
  n = length(y)
  suma = 0
  for (i in 1:n){
    suma = suma + y[i]*sum(X[i,]*beta) - 
      log(1 + exp( sum(t(X[i,])*beta) ))
  }
  return(suma)
}
```

Por ejemplo, para $\beta_0 = -2$ y $\beta_1 = \beta_2 = \beta_3 = \beta_4 = 0.5$, la función de verosimilitud vale:

```{r}
beta = c(-2,0.05,0.05,0.05,0.05)
X = cbind(rep(1,nrow(d)), d[,3:6])
logL(beta,d$InMichelin,X)
```

## El máximo de la función de verosimilitud

Derivando e igualando a cero:

$$
\frac{\partial logL(\beta)}{\partial \beta} 
=
\begin{bmatrix}
\frac{\partial logL(\beta)}{\partial \beta_0} \\ 
\frac{\partial logL(\beta)}{\partial \beta_1} \\
\cdots \\
\frac{\partial logL(\beta)}{\partial \beta_k} \\
\end{bmatrix}
= 
X^T(y - \lambda)
=
\begin{bmatrix}
0 \\
0 \\
\cdots \\
0
\end{bmatrix}
$$

donde $X$:

$$
X = 
\begin{bmatrix}
1 & x_{11} & \cdots & x_{k1} \\
1 & x_{12} & \cdots & x_{k2} \\
\cdots & \cdots & \cdots & \cdots \\
1 & x_{1n} & \cdots & x_{kn} \\
\end{bmatrix}
, \quad
y = 
\begin{bmatrix}
y_1 \\ y_2 \\ \cdots \\ y_n
\end{bmatrix}
, \quad
\lambda = 
\begin{bmatrix}
\lambda_1 \\ \lambda_2 \\ \cdots \\ \lambda_n
\end{bmatrix}
$$

El máximo de la función log-verosimilitud se tiene que hacer numéricamente.

En los siguientes apartados se va a necesitar la matriz de derivadas segundas o matriz hessiana. Su valor es:

$$
\frac{\partial log L(\beta)}{\partial \beta \partial \beta^T}
=
\begin{bmatrix}
\frac{\partial^2 logL(\beta)}{\partial \beta_0^2} &  \frac{\partial^2 logL(\beta)}{\partial \beta_0 \partial \beta_1} & \cdots & \frac{\partial^2 logL(\beta)}{\partial \beta_0 \partial \beta_k} \\
\frac{\partial^2 logL(\beta)}{\partial \beta_1 \partial \beta_0} & \frac{\partial^2 logL(\beta)}{\partial \beta_1^2} & \cdots & \frac{\partial^2 logL(\beta)}{\partial \beta_1 \partial \beta_k} \\
\cdots & \cdots & \cdots & \cdots \\
\frac{\partial^2 logL(\beta)}{\partial \beta_k \partial \beta_0} & \frac{\partial^2 logL(\beta)}{\partial \beta_k \partial \beta_1 } & \cdots & \frac{\partial^2 logL(\beta)}{\partial \beta_k^2}
\end{bmatrix}
=
- X^T W X
$$

donde $W$ es una matriz diagonal con

$$
W_{ii} = \lambda_i
$$

En R:

```{r}
grad_logL = function(beta,y,X){
  X = as.matrix(X)
  n = length(y)
  y = matrix(y, nrow = n, ncol = 1)
  pi = matrix(0, nrow = n, ncol = 1)
  for (i in 1:n){
    pi[i,1] = exp(sum(X[i,]*beta))/(1 + exp(sum(X[i,]*beta)))
  }
  grad = t(X) %*% (y - pi)
  return(grad)
}
```

Comprobacion:

```{r}
beta = c(-2,0.05,0.05,0.05,0.05)
X = cbind(rep(1,nrow(d)), d[,3:6])
grad_logL(beta, d$InMichelin, X)
```

```{r}
hess_logL = function(beta,X){
  X = as.matrix(X)
  n = nrow(X)
  W = matrix(0, nrow = n, ncol = n)
  for (i in 1:n){
    pi = exp(sum(X[i,]*beta))/(1 + exp(sum(X[i,]*beta)))
    W[i,i] = pi*(1-pi)
  }
  hess = - t(X) %*% W %*% X
  return(hess)
}
```

```{r}
beta = c(-2,0.05,0.05,0.05,0.05)
X = cbind(rep(1,nrow(d)), d[,3:6])
hess_logL(beta, X)
nlme::fdHess(beta,logL, y = d$InMichelin, X)
```

Las funciones que calculan el logaritmo de la verosimilitud, el gradiente y el hessiano se han incluido en el archivo (poisson_funciones.R)

## Algoritmo de Newton-Raphson

El algoritmo de Newton-Raphson para la función log-verosimilitud es:

$$
\beta_{k+1} = \beta_k - \alpha H^{-1}_k G_k
$$

donde $\beta = [\beta_0 \ \beta_1 \ \cdots \beta_k]^T$. Este algoritmo para la función log-verosimilitud se puede implementar en R de manera sencilla:

```{r}
Newton_logL = function(beta_i, y, X, max_iter = 100, tol = 10^(-6), alfa = 0.1){
  
  # punto de partida
  beta = beta_i
  
  iter = 1
  tol1 = Inf
  while ((iter <= max_iter) & (tol1 > tol)){
    f = logL(beta,y,X)
    grad = grad_logL(beta,y,X)
    hess = hess_logL(beta,X)
    beta = beta - alfa*solve(hess) %*% grad
    f1 = logL(beta,y,X)
    tol1 = abs((f1-f)/f)
    print(paste("Iteracion ",iter," log-verosimilitud ",f1))
    iter = iter + 1
  }
  return(beta)
}
```

Como punto de partida podemos utilizar por ejemplo la solución de mínimos cuadrados:

```{r}
m = lm(InMichelin ~ Food + Decor + Service + Price, data = d)
beta_i = coef(m)
X = cbind(rep(1,nrow(d)), d[,3:6])
Newton_logL(beta_i,d$InMichelin,X)
```

## Algoritmo BFGS

La función que vamos a minimizar es:

```{r}
logLa = function(beta,y,X){
  logL = logL(beta,y,X)
  return(-logL)
}
```

Utilizando el mismo punto de partida que para el algoritmo Newton:

```{r}
mle = optim(par = beta_i, fn = logLa, y = d$InMichelin, X = X, gr = NULL, method = "BFGS", hessian = TRUE, control = list(trace=1, REPORT = 1, maxit = 200))
mle$par
```

## Estimacion con R

```{r}
d$bomber = factor(d$bomber, labels = c("A4","A6"))
m2 = glm(damage ~ bomber + load + experience, data = d, family = poisson)
summary(m2)
```
